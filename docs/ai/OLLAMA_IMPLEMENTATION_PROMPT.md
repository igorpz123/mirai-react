# ü§ñ Prompt para Implementa√ß√£o Completa do Ollama

## Objetivo
Implementar integra√ß√£o com **Ollama (IA local)** no sistema Mirai React, mantendo **compatibilidade com a estrutura atual** do Gemini e criando um **sistema h√≠brido** com fallback autom√°tico.

## Contexto T√©cnico
- **Stack:** React + Vite + Express + TypeScript + Socket.IO
- **Estrutura atual:** Sistema de IA usando Google Gemini em `server/services/aiService.ts`
- **Funcionalidades existentes:** Gera√ß√£o de texto, an√°lise de imagem, chat multi-turno, cache, rate limiting, logs de tokens
- **Arquivos relacionados:** 
  - `server/services/aiService.ts` (service principal)
  - `server/controllers/AIController.ts` (endpoints)
  - `server/routes/ai.ts` (rotas)
  - `server/middleware/rateLimiter.ts` (rate limiting)

## Requisitos da Implementa√ß√£o

### 1. Criar Novo Service com Ollama (`server/services/aiService.ollama.ts`)

**Funcionalidades obrigat√≥rias:**
- ‚úÖ `generateText(userId, prompt)` - Usar modelo `llama3.2` ou `mistral`
- ‚úÖ `analyzeImage(userId, base64Image, prompt)` - Usar modelo `llava` para vis√£o computacional
- ‚úÖ `chatMultiTurn(userId, message, history)` - Chat com contexto de hist√≥rico
- ‚úÖ **Retry com backoff exponencial** (3 tentativas, delays: 1s, 2s, 4s)
- ‚úÖ **Cache de respostas** (Map<string, CacheEntry> com TTL de 15min)
- ‚úÖ **Logs de consumo** (tracking de tokens estimados e performance)
- ‚úÖ **Timeout de 30 segundos** por requisi√ß√£o
- ‚úÖ **Sanitiza√ß√£o de inputs** (remover caracteres de controle, limite de 30k caracteres)
- ‚úÖ **Tratamento de erros espec√≠ficos** (timeout, connection refused, rate limit)

**API Ollama:**
```typescript
// Endpoint: POST http://localhost:11434/api/generate
// Corpo para texto:
{
  model: 'llama3.2',
  prompt: string,
  stream: false,
  options: { temperature: 0.7, num_predict: 2048 }
}

// Corpo para imagem:
{
  model: 'llava',
  prompt: string,
  images: [base64String], // sem prefixo data:image/...
  stream: false
}

// Resposta:
{
  response: string,
  model: string,
  created_at: string,
  done: boolean
}
```

**Vari√°veis de ambiente (adicionar ao `.env`):**
```bash
# Configura√ß√£o de IA
AI_PROVIDER=ollama          # 'ollama' ou 'gemini'
OLLAMA_URL=http://localhost:11434
OLLAMA_TEXT_MODEL=llama3.2
OLLAMA_VISION_MODEL=llava
OLLAMA_TIMEOUT=30000        # 30 segundos
```

### 2. Criar Service H√≠brido (`server/services/aiService.hybrid.ts`)

**L√≥gica de fallback:**
```typescript
1. Detectar AI_PROVIDER do .env ('ollama' ou 'gemini')
2. Se 'ollama':
   - Tentar Ollama primeiro
   - Se falhar (ECONNREFUSED, timeout, erro 500):
     ‚Üí Log de warning
     ‚Üí Fallback autom√°tico para Gemini
     ‚Üí Adicionar flag 'usedFallback: true' na resposta
3. Se 'gemini':
   - Usar apenas Gemini (comportamento atual)
```

**Interface comum:**
```typescript
export interface AIResponse<T> {
  data: T
  cached: boolean
  provider: 'ollama' | 'gemini'
  usedFallback?: boolean
  responseTime?: number
}
```

### 3. Atualizar Controller (`server/controllers/AIController.ts`)

**Mudan√ßas necess√°rias:**
- ‚úÖ Importar `aiService.hybrid.ts` ao inv√©s de `aiService.ts`
- ‚úÖ Retornar campo adicional `provider` em todas as respostas
- ‚úÖ Adicionar campo `usedFallback` quando fallback for usado
- ‚úÖ Adicionar campo `responseTime` (tempo de execu√ß√£o em ms)
- ‚úÖ Manter compatibilidade com frontend existente

**Exemplo de resposta atualizada:**
```json
{
  "text": "...",
  "cached": false,
  "provider": "ollama",
  "usedFallback": false,
  "responseTime": 1234,
  "timestamp": "2025-11-05T..."
}
```

### 4. Criar Endpoint de Health Check (`GET /api/ai/health`)

**Resposta esperada:**
```json
{
  "ollama": {
    "available": true,
    "url": "http://localhost:11434",
    "models": ["llama3.2", "llava"],
    "responseTime": 45
  },
  "gemini": {
    "available": true,
    "configured": true
  },
  "currentProvider": "ollama",
  "fallbackEnabled": true
}
```

**Implementa√ß√£o:**
- Testar conex√£o com Ollama via `GET /api/tags`
- Verificar se `GEMINI_API_KEY` est√° configurada
- Retornar status de ambos os providers

### 5. Criar Scripts de Setup

**Script 1: `server/scripts/setup-ollama.sh`**
```bash
#!/bin/bash
# Instalar Ollama e baixar modelos necess√°rios

echo "ü§ñ Instalando Ollama..."
curl -fsSL https://ollama.com/install.sh | sh

echo "üì¶ Baixando modelos..."
ollama pull llama3.2    # ~2GB - Texto
ollama pull llava       # ~4.7GB - Vis√£o
ollama pull mistral     # ~4GB - Alternativa

echo "‚úÖ Ollama configurado! Servidor rodando em localhost:11434"
```

**Script 2: `server/scripts/test-ollama.sh`**
```bash
#!/bin/bash
# Testar se Ollama est√° funcionando

echo "üß™ Testando Ollama..."
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Hello, world!",
  "stream": false
}'
```

### 6. Atualizar Documenta√ß√£o

**Criar arquivo: `docs/ai/OLLAMA_SETUP.md`**

Conte√∫do:
- ‚úÖ Passo a passo de instala√ß√£o do Ollama
- ‚úÖ Comandos para baixar modelos
- ‚úÖ Configura√ß√£o de vari√°veis de ambiente
- ‚úÖ Troubleshooting comum (porta ocupada, modelos n√£o encontrados, out of memory)
- ‚úÖ Compara√ß√£o de performance Ollama vs Gemini
- ‚úÖ Requisitos de hardware (RAM, CPU, GPU opcional)

**Atualizar arquivo: `docs/ai/README.md`**
- Adicionar se√ß√£o sobre Ollama
- Documentar sistema h√≠brido e fallback
- Exemplos de uso com cada provider

### 7. Adicionar Testes no Frontend (Opcional)

**Criar: `src/pages/AITest.tsx`**
- Interface para testar ambos os providers
- Bot√£o "Testar Ollama" e "Testar Gemini"
- Exibir tempo de resposta e provider usado
- Mostrar status de fallback

### 8. Considera√ß√µes de Performance

**Otimiza√ß√µes obrigat√≥rias:**
- ‚úÖ **Streaming desabilitado** (Ollama suporta, mas simplifica implementa√ß√£o inicial)
- ‚úÖ **Context size:** Limitar a 4096 tokens para evitar OOM
- ‚úÖ **Concurrency:** M√°ximo 3 requisi√ß√µes simult√¢neas ao Ollama (via Semaphore)
- ‚úÖ **Cache agressivo:** Respostas id√™nticas em 15min n√£o reprocessam
- ‚úÖ **Timeout:** 30s para texto, 60s para imagem (vis√£o √© mais lenta)

### 9. Tratamento de Erros Espec√≠ficos

**Erros do Ollama:**
```typescript
// ECONNREFUSED - Ollama n√£o est√° rodando
‚Üí "Ollama n√£o est√° dispon√≠vel. Usando Gemini como fallback."

// Model not found - Modelo n√£o baixado
‚Üí "Modelo X n√£o encontrado. Execute: ollama pull X"

// Out of memory - RAM insuficiente
‚Üí "Mem√≥ria insuficiente para processar. Tente um prompt menor."

// Timeout
‚Üí "Requisi√ß√£o demorou muito. Usando Gemini como fallback."
```

### 10. Logs e Monitoramento

**Logs obrigat√≥rios:**
```typescript
console.log('[Ollama] Tentando gerar texto com llama3.2...')
console.log('[Ollama] Resposta recebida em 1234ms')
console.warn('[Ollama] Falha na conex√£o, usando fallback para Gemini')
console.error('[Ollama] Erro inesperado:', error)
```

**Estat√≠sticas de uso (adicionar ao `/api/ai/stats`):**
```json
{
  "totalRequests": 150,
  "ollamaRequests": 120,
  "geminiRequests": 30,
  "fallbackCount": 15,
  "averageResponseTime": {
    "ollama": 1234,
    "gemini": 890
  }
}
```

## Checklist de Implementa√ß√£o

- [ ] Criar `server/services/aiService.ollama.ts` com API do Ollama
- [ ] Criar `server/services/aiService.hybrid.ts` com l√≥gica de fallback
- [ ] Atualizar `server/controllers/AIController.ts` para usar h√≠brido
- [ ] Adicionar endpoint `GET /api/ai/health`
- [ ] Criar scripts `setup-ollama.sh` e `test-ollama.sh`
- [ ] Adicionar vari√°veis de ambiente ao `.env.example`
- [ ] Criar documenta√ß√£o em `docs/ai/OLLAMA_SETUP.md`
- [ ] Atualizar `docs/ai/README.md`
- [ ] Testar gera√ß√£o de texto com Ollama
- [ ] Testar an√°lise de imagem com llava
- [ ] Testar fallback autom√°tico (parar Ollama e verificar se usa Gemini)
- [ ] Testar cache e rate limiting com ambos os providers
- [ ] Verificar logs e estat√≠sticas de uso

## Crit√©rios de Sucesso

‚úÖ **Funcional:** Sistema gera texto e analisa imagens via Ollama  
‚úÖ **Compat√≠vel:** Frontend n√£o precisa de altera√ß√µes (mesma API)  
‚úÖ **Robusto:** Fallback autom√°tico para Gemini em caso de falha  
‚úÖ **Monitorado:** Logs claros e estat√≠sticas de uso por provider  
‚úÖ **Documentado:** Setup completo e troubleshooting no docs/  

## Exemplo de Uso Final

```typescript
// Backend decide automaticamente qual provider usar
const result = await generateText(userId, "Explique TypeScript")

// Resposta:
{
  text: "TypeScript √© uma linguagem...",
  cached: false,
  provider: "ollama",      // ou "gemini" se usou fallback
  usedFallback: false,
  responseTime: 1234
}
```

---

**üéØ Este prompt est√° pronto para ser usado com o GitHub Copilot Coding Agent ou para implementa√ß√£o manual passo a passo.**
