# ğŸ¤– DocumentaÃ§Ã£o de IntegraÃ§Ã£o com IA

Esta pasta contÃ©m toda a documentaÃ§Ã£o relacionada Ã  integraÃ§Ã£o com **IA no Mirai React**.

O sistema suporta **dois providers de IA**:
- ğŸŒ **Google Gemini** (IA na nuvem)
- ğŸ  **Ollama** (IA local)

Com **sistema hÃ­brido** e **fallback automÃ¡tico** entre providers.

---

## ğŸ“š Guias DisponÃ­veis

### [OLLAMA_SETUP.md](OLLAMA_SETUP.md) ğŸ  **NOVO!**
**Guia completo de instalaÃ§Ã£o do Ollama** (300+ linhas)

Setup completo de IA local incluindo:
- âœ… InstalaÃ§Ã£o do Ollama (Linux, macOS, Windows)
- âœ… Download de modelos (llama3.2, llava, mistral)
- âœ… ConfiguraÃ§Ã£o do sistema hÃ­brido
- âœ… Fallback automÃ¡tico para Gemini
- âœ… ComparaÃ§Ã£o de performance
- âœ… Troubleshooting detalhado
- âœ… Requisitos de hardware

**Para quem:** Desenvolvedores que querem rodar IA localmente com privacidade e sem custos.

---

### [AI_SETUP.md](AI_SETUP.md) ğŸ“–
**Guia completo de configuraÃ§Ã£o do Gemini** (260 linhas)

DocumentaÃ§Ã£o abrangente incluindo:
- âœ… InstalaÃ§Ã£o de dependÃªncias backend (`@google/generative-ai`)
- âœ… ImplementaÃ§Ã£o do serviÃ§o de IA (`aiService.ts`)
- âœ… CriaÃ§Ã£o de controllers e rotas
- âœ… ImplementaÃ§Ã£o do frontend (pÃ¡gina de chat)
- âœ… ConfiguraÃ§Ã£o de variÃ¡veis de ambiente
- âœ… Troubleshooting detalhado

**Para quem:** Desenvolvedores fazendo setup inicial ou entendendo a arquitetura completa.

---

### [AI_QUICKSTART.md](AI_QUICKSTART.md) âš¡
**InÃ­cio rÃ¡pido em 3 passos** (110 linhas)

Guia resumido para setup em 5 minutos:
1. Instalar dependÃªncia
2. Configurar API Key
3. Testar integraÃ§Ã£o

**Para quem:** Desenvolvedores que querem comeÃ§ar rapidamente.

---

### [AI_PROMPT_EXAMPLES.md](AI_PROMPT_EXAMPLES.md) ğŸ’¡
**20+ exemplos prÃ¡ticos de uso** (300+ linhas)

Exemplos organizados por categoria:
- ğŸ“Š AnÃ¡lise de dados e relatÃ³rios
- âœï¸ GeraÃ§Ã£o de conteÃºdo
- ğŸ–¼ï¸ AnÃ¡lise de imagens
- ğŸ“‹ IntegraÃ§Ã£o com checklists
- ğŸ’¬ SugestÃµes de chat

**Para quem:** Desenvolvedores e usuÃ¡rios finais buscando inspiraÃ§Ã£o.

---

### [GEMINI_API_KEY_GUIDE.md](GEMINI_API_KEY_GUIDE.md) ğŸ”‘
**Passo a passo para gerar API Key**

InstruÃ§Ãµes detalhadas:
- Como acessar Google AI Studio
- Criar projeto e gerar chave
- Configurar no projeto
- Validar a chave com script de teste

**Para quem:** Qualquer pessoa que precise gerar uma API Key do Google Gemini.

---

## ğŸš€ ComeÃ§ando

### Escolha seu Provider

#### OpÃ§Ã£o 1: Ollama (IA Local) ğŸ 
**Recomendado se:** VocÃª tem servidor com â‰¥8GB RAM e quer privacidade/custo zero.

```bash
# 1. Instalar Ollama e modelos
cd server/scripts
./setup-ollama.sh

# 2. Configurar .env
AI_PROVIDER=ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_TEXT_MODEL=llama3.2
OLLAMA_VISION_MODEL=llava

# 3. Testar
./test-ollama.sh
```

**Guia completo:** [OLLAMA_SETUP.md](OLLAMA_SETUP.md)

---

#### OpÃ§Ã£o 2: Google Gemini (IA na Nuvem) â˜ï¸
**Recomendado se:** VocÃª quer setup rÃ¡pido e nÃ£o tem hardware adequado.

```bash
# 1. Instalar dependÃªncia
cd server && npm install @google/generative-ai

# 2. Configurar .env
AI_PROVIDER=gemini
GEMINI_API_KEY=sua_chave_aqui
GEMINI_MODEL=gemini-2.5-flash

# 3. Testar
export $(cat .env | grep -v '^#' | xargs)
node test-gemini-key.js
```

**Guia completo:** [AI_SETUP.md](AI_SETUP.md)

---

#### OpÃ§Ã£o 3: Sistema HÃ­brido (Recomendado) ğŸ”„
**Ollama como principal + Gemini como fallback**

```bash
# Configurar .env
AI_PROVIDER=ollama
OLLAMA_URL=http://localhost:11434
GEMINI_API_KEY=sua_chave_aqui  # Para fallback

# Sistema usarÃ¡ Ollama e farÃ¡ fallback para Gemini se:
# - Ollama nÃ£o responder (ECONNREFUSED)
# - Timeout (>30s)
# - Erro interno (500)
```

---

## ğŸ¯ Funcionalidades Implementadas

### âœ¨ Backend (`server/`)

**Arquitetura HÃ­brida:**
- **`services/aiService.ts`** - ServiÃ§o Gemini (original)
- **`services/aiService.ollama.ts`** - ServiÃ§o Ollama (novo)
- **`services/aiService.hybrid.ts`** - Gerenciador hÃ­brido com fallback automÃ¡tico

**3 MÃ©todos Principais:**
- `generateText()` - GeraÃ§Ã£o de texto
- `analyzeImage()` - AnÃ¡lise de imagens base64
- `chatMultiTurn()` - Chat com histÃ³rico multi-turno

**Features AvanÃ§adas:**
- âš¡ Cache de respostas (15 min, 500 entradas)
- ğŸ”„ Retry com backoff exponencial (3 tentativas)
- ğŸ“Š Logging de tokens consumidos
- ğŸš¦ Rate limiting (100 req/min por usuÃ¡rio)
- ğŸ›¡ï¸ SanitizaÃ§Ã£o de inputs
- ğŸ”€ Fallback automÃ¡tico entre providers
- ğŸ­ Controle de concorrÃªncia (3 requisiÃ§Ãµes Ollama simultÃ¢neas)
- âŒ Error handling robusto

### ğŸ¨ Frontend (`src/`)
- **`pages/AIChat.tsx`** - Interface de chat moderna:
  - ğŸ’¬ Conversas multi-turno
  - âš¡ Indicador de cache
  - ğŸš¦ Display de rate limit
  - ğŸ“± Design responsivo
  - ğŸŒ™ Suporte dark mode
  - âœ¨ RenderizaÃ§Ã£o de Markdown

---

## ğŸ“Š API Endpoints

Todos os endpoints estÃ£o em `/api/ai/`:

| Endpoint | MÃ©todo | DescriÃ§Ã£o | Resposta Inclui Provider |
|----------|--------|-----------|--------------------------|
| `/api/ai/health` | GET | Health check dos providers | âœ… |
| `/api/ai/text` | POST | GeraÃ§Ã£o de texto simples | âœ… provider, usedFallback, responseTime |
| `/api/ai/image` | POST | AnÃ¡lise de imagem (base64) | âœ… provider, usedFallback, responseTime |
| `/api/ai/chat` | POST | Chat multi-turno | âœ… provider, usedFallback, responseTime |
| `/api/ai/stats` | GET | EstatÃ­sticas de uso | âœ… Separado por provider |
| `/api/ai/cache/clear` | POST | Limpar cache | - |

### Exemplo de Resposta HÃ­brida

```json
{
  "text": "TypeScript Ã© uma linguagem...",
  "cached": false,
  "provider": "ollama",
  "usedFallback": false,
  "responseTime": 1234,
  "timestamp": "2025-11-05T..."
}
```

### Health Check Response

```json
{
  "ollama": {
    "available": true,
    "url": "http://localhost:11434",
    "models": ["llama3.2", "llava"],
    "responseTime": 45
  },
  "gemini": {
    "available": true,
    "configured": true
  },
  "currentProvider": "ollama",
  "fallbackEnabled": true
}
```

---

## ğŸ”§ Troubleshooting

### Ollama
â¡ï¸ Veja [OLLAMA_SETUP.md - Troubleshooting](OLLAMA_SETUP.md#troubleshooting)

Problemas comuns:
- Servidor nÃ£o responde â†’ Execute `ollama serve`
- Modelo nÃ£o encontrado â†’ Execute `ollama pull llama3.2`
- Out of memory â†’ Use modelo menor ou aumente RAM
- Timeout â†’ Aumente `OLLAMA_TIMEOUT` no .env

### Gemini

### Erro 401 "API keys are not supported by this API"
â¡ï¸ VocÃª estÃ¡ usando chave do Vertex AI. Precisa gerar no [Google AI Studio](https://aistudio.google.com/apikey).

### Erro 404 "Model not found"
â¡ï¸ Modelo desatualizado. Use `gemini-2.5-flash` (versÃ£o atual).

### Erro 429 "Rate limit exceeded"
â¡ï¸ Limite de 100 req/min por usuÃ¡rio atingido. Aguarde 1 minuto.

### Mais soluÃ§Ãµes
Veja seÃ§Ã£o completa de troubleshooting em [AI_SETUP.md](AI_SETUP.md).

---

## ğŸ”— Links Ãšteis

### Ollama
- [Ollama Official](https://ollama.com/) - Website oficial
- [Model Library](https://ollama.com/library) - CatÃ¡logo de modelos
- [GitHub](https://github.com/ollama/ollama) - CÃ³digo fonte
- [API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md) - DocumentaÃ§Ã£o da API

### Gemini
- [Google AI Studio](https://aistudio.google.com/) - Gerar API Keys
- [Gemini API Docs](https://ai.google.dev/docs) - DocumentaÃ§Ã£o oficial
- [Pricing](https://ai.google.dev/pricing) - PreÃ§os e limites

---

## ğŸ“ Notas

**Providers Suportados:**
- Ollama (local) - llama3.2, llava, mistral, etc.
- Google Gemini (cloud) - gemini-2.5-flash

**VersÃµes:**
- Modelo Gemini atual: `gemini-2.5-flash` (junho 2025)
- SDK Gemini: `@google/generative-ai` v0.21.0+
- Ollama: 0.1.0+

**ConfiguraÃ§Ãµes PadrÃ£o:**
- Rate limit: 100 req/min por usuÃ¡rio (configurÃ¡vel)
- Cache TTL: 15 minutos (configurÃ¡vel)
- Ollama timeout: 30s texto, 60s imagem
- Ollama concurrency: 3 requisiÃ§Ãµes simultÃ¢neas

**Custos:**
- **Ollama:** Gratuito (custos de infraestrutura)
- **Gemini Free tier:** 15 RPM, 1M TPM, 1500 RPD
- **Gemini Paid tier:** Consulte [pricing oficial](https://ai.google.dev/pricing)

**Sistema HÃ­brido:**
- Provider principal configurÃ¡vel via `AI_PROVIDER`
- Fallback automÃ¡tico de Ollama â†’ Gemini
- Cache separado por provider
- Logs unificados com tag de provider

---

## ğŸ†• Changelog

### v2.0 (Novembro 2025)
- âœ¨ Adicionado suporte ao Ollama (IA local)
- âœ¨ Sistema hÃ­brido com fallback automÃ¡tico
- âœ¨ Controle de concorrÃªncia para Ollama
- âœ¨ Health check endpoint (`/api/ai/health`)
- âœ¨ Provider metadata em todas as respostas
- âœ¨ Scripts de instalaÃ§Ã£o e teste
- ğŸ“š DocumentaÃ§Ã£o completa do Ollama
- ğŸ› Compatibilidade mantida com frontend existente

### v1.0 (Original)
- âœ¨ IntegraÃ§Ã£o com Google Gemini
- âœ¨ Cache de respostas
- âœ¨ Rate limiting
- âœ¨ Retry com backoff
- âœ¨ Chat multi-turno
- âœ¨ AnÃ¡lise de imagens

---

ğŸ“š Para mais detalhes, veja [documentaÃ§Ã£o completa](../INDEX.md).
