# ü§ñ Guia de Setup do Ollama no Mirai React

Este guia completo ensina como instalar e configurar o **Ollama** (IA local) no sistema Mirai React, com suporte h√≠brido e fallback autom√°tico para o Google Gemini.

## üìã √çndice

1. [O que √© Ollama?](#o-que-√©-ollama)
2. [Requisitos de Hardware](#requisitos-de-hardware)
3. [Instala√ß√£o](#instala√ß√£o)
4. [Configura√ß√£o](#configura√ß√£o)
5. [Uso e Teste](#uso-e-teste)
6. [Compara√ß√£o Ollama vs Gemini](#compara√ß√£o-ollama-vs-gemini)
7. [Troubleshooting](#troubleshooting)
8. [FAQ](#faq)

---

## O que √© Ollama?

**Ollama** √© uma plataforma que permite rodar modelos de IA grandes (LLMs) **localmente** em sua pr√≥pria m√°quina, sem depender de APIs externas. Principais benef√≠cios:

- ‚úÖ **Privacidade:** Dados n√£o saem do seu servidor
- ‚úÖ **Sem custos:** N√£o h√° cobran√ßa por tokens ou requisi√ß√µes
- ‚úÖ **Offline:** Funciona sem conex√£o com internet
- ‚úÖ **Controle total:** Voc√™ gerencia os modelos e recursos

O Mirai React implementa um **sistema h√≠brido** que pode usar Ollama como provider principal e fazer fallback autom√°tico para Gemini em caso de falhas.

---

## Requisitos de Hardware

### M√≠nimos (Funcional)
- **RAM:** 8 GB
- **Armazenamento:** 15 GB livres
- **CPU:** Qualquer processador moderno (x64)
- **GPU:** Opcional (acelera processamento)

### Recomendados (Performance Ideal)
- **RAM:** 16 GB ou mais
- **Armazenamento:** 30 GB livres (SSD prefer√≠vel)
- **CPU:** 4+ cores
- **GPU:** NVIDIA com 6+ GB VRAM (CUDA) ou Apple Silicon (M1/M2)

### Requisitos por Modelo

| Modelo | Tamanho | RAM M√≠nima | Uso |
|--------|---------|------------|-----|
| `llama3.2` | ~2 GB | 4 GB | Gera√ß√£o de texto |
| `llava` | ~4.7 GB | 8 GB | An√°lise de imagem |
| `mistral` | ~4 GB | 8 GB | Texto (alternativa) |

---

## Instala√ß√£o

### M√©todo 1: Script Automatizado (Linux)

```bash
# No diret√≥rio raiz do projeto
cd server/scripts

# Executar script de instala√ß√£o
./setup-ollama.sh
```

O script ir√°:
1. Instalar o Ollama
2. Baixar modelos necess√°rios (llama3.2, llava, mistral)
3. Iniciar o servidor Ollama
4. Exibir instru√ß√µes de configura√ß√£o

### M√©todo 2: Instala√ß√£o Manual

#### Linux

```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Iniciar servidor
ollama serve

# Em outro terminal, baixar modelos
ollama pull llama3.2    # ~2GB - Texto
ollama pull llava       # ~4.7GB - Vis√£o
ollama pull mistral     # ~4GB - Alternativa
```

#### macOS

```bash
# Op√ß√£o 1: Download direto
# Baixe em: https://ollama.com/download

# Op√ß√£o 2: Homebrew
brew install ollama

# Iniciar servidor
ollama serve

# Baixar modelos
ollama pull llama3.2
ollama pull llava
ollama pull mistral
```

#### Windows

```bash
# Baixe o instalador em:
# https://ollama.com/download

# Ap√≥s instala√ß√£o, abra PowerShell e baixe modelos:
ollama pull llama3.2
ollama pull llava
ollama pull mistral
```

---

## Configura√ß√£o

### 1. Vari√°veis de Ambiente

Edite `server/.env` e adicione:

```bash
# Configura√ß√£o de IA
AI_PROVIDER=ollama          # 'ollama' ou 'gemini'
OLLAMA_URL=http://localhost:11434
OLLAMA_TEXT_MODEL=llama3.2
OLLAMA_VISION_MODEL=llava
OLLAMA_TIMEOUT=30000        # 30 segundos

# Gemini (para fallback)
GEMINI_API_KEY=sua_chave_aqui   # Opcional, mas recomendado
GEMINI_MODEL=gemini-2.5-flash
```

### 2. Modos de Opera√ß√£o

**Modo 1: Apenas Ollama (sem fallback)**
```bash
AI_PROVIDER=ollama
# N√£o configure GEMINI_API_KEY
```
‚Üí Sistema usar√° apenas Ollama. Erros retornam falha direta ao frontend.

**Modo 2: Ollama com fallback para Gemini (Recomendado)**
```bash
AI_PROVIDER=ollama
GEMINI_API_KEY=sua_chave_aqui
```
‚Üí Sistema tenta Ollama primeiro. Se falhar (conex√£o, timeout, erro 500), usa Gemini automaticamente.

**Modo 3: Apenas Gemini (padr√£o atual)**
```bash
AI_PROVIDER=gemini
GEMINI_API_KEY=sua_chave_aqui
```
‚Üí Sistema usa apenas Gemini (comportamento original).

---

## Uso e Teste

### 1. Testar Instala√ß√£o

```bash
# Executar script de teste
cd server/scripts
./test-ollama.sh
```

Sa√≠da esperada:
```
‚úì Servidor Ollama est√° rodando
‚úì Modelos instalados: llama3.2, llava, mistral
‚úì Gera√ß√£o de texto funcionando
‚úì An√°lise de imagem funcionando
```

### 2. Iniciar Servidor Node.js

```bash
cd server
npm run dev
```

### 3. Verificar Health Check

```bash
curl http://localhost:5000/api/ai/health
```

Resposta esperada:
```json
{
  "ollama": {
    "available": true,
    "url": "http://localhost:11434",
    "models": ["llama3.2", "llava", "mistral"],
    "responseTime": 45
  },
  "gemini": {
    "available": true,
    "configured": true
  },
  "currentProvider": "ollama",
  "fallbackEnabled": true
}
```

### 4. Testar Gera√ß√£o de Texto

```bash
curl -X POST http://localhost:5000/api/ai/text \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer SEU_TOKEN_JWT" \
  -d '{
    "prompt": "Explique TypeScript em uma frase"
  }'
```

Resposta esperada:
```json
{
  "text": "TypeScript √© uma linguagem...",
  "cached": false,
  "provider": "ollama",
  "usedFallback": false,
  "responseTime": 1234,
  "timestamp": "2025-11-05T..."
}
```

### 5. Testar Fallback Autom√°tico

```bash
# Parar servidor Ollama
pkill ollama

# Fazer requisi√ß√£o (deve usar Gemini como fallback)
curl -X POST http://localhost:5000/api/ai/text \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer SEU_TOKEN_JWT" \
  -d '{"prompt": "Teste de fallback"}'
```

Resposta com fallback:
```json
{
  "text": "...",
  "provider": "gemini",
  "usedFallback": true,
  "responseTime": 890
}
```

---

## Compara√ß√£o Ollama vs Gemini

| Aspecto | Ollama | Gemini |
|---------|--------|--------|
| **Custo** | Gratuito (infra pr√≥pria) | Pago por token |
| **Privacidade** | Total (dados locais) | Dados v√£o para Google |
| **Velocidade** | Depende do hardware | Geralmente r√°pido |
| **Disponibilidade** | 99.9% (se infra OK) | 99.9% (SLA do Google) |
| **Qualidade** | Boa (modelos open) | Excelente (SOTA) |
| **Requisitos** | Servidor com RAM/CPU | Apenas API Key |
| **Offline** | ‚úÖ Funciona offline | ‚ùå Requer internet |
| **Setup** | Mais complexo | Simples (s√≥ API key) |

### Quando usar cada um?

**Use Ollama como principal se:**
- Precisa de privacidade total
- Tem infraestrutura adequada (RAM/CPU)
- Quer evitar custos de API
- Trabalha com dados sens√≠veis

**Use Gemini como principal se:**
- Prioriza facilidade de setup
- N√£o tem hardware adequado para Ollama
- Precisa da melhor qualidade poss√≠vel
- Volume de requisi√ß√µes justifica o custo

**Use sistema h√≠brido (Recomendado) se:**
- Quer melhor custo-benef√≠cio
- Precisa de alta disponibilidade
- Quer aproveitar o melhor dos dois mundos

---

## Troubleshooting

### Problema 1: "Ollama n√£o est√° dispon√≠vel"

**Sintomas:**
```
Error: ECONNREFUSED - Ollama n√£o est√° dispon√≠vel
```

**Solu√ß√µes:**
```bash
# Verificar se servidor est√° rodando
curl http://localhost:11434/api/tags

# Se n√£o responder, iniciar servidor
ollama serve

# Verificar porta correta no .env
OLLAMA_URL=http://localhost:11434
```

---

### Problema 2: "Modelo n√£o encontrado"

**Sintomas:**
```
Error: Model 'llama3.2' not found
```

**Solu√ß√µes:**
```bash
# Baixar modelo faltante
ollama pull llama3.2

# Verificar modelos instalados
ollama list

# Ajustar modelo no .env se necess√°rio
OLLAMA_TEXT_MODEL=llama3.2
```

---

### Problema 3: "Out of memory"

**Sintomas:**
- Processo Ollama √© morto (killed)
- Sistema fica lento/travando
- Erros de aloca√ß√£o de mem√≥ria

**Solu√ß√µes:**
```bash
# Usar modelo menor
ollama pull phi3  # Apenas 2.3GB, requer 4GB RAM

# Ajustar no .env
OLLAMA_TEXT_MODEL=phi3

# Alternativa: Aumentar RAM do servidor
# Ou usar Gemini como fallback para requisi√ß√µes grandes
```

---

### Problema 4: Timeout nas requisi√ß√µes

**Sintomas:**
```
Error: Request timeout after 30000ms
```

**Solu√ß√µes:**
```bash
# Aumentar timeout no .env
OLLAMA_TIMEOUT=60000  # 60 segundos

# Verificar carga do servidor
top | grep ollama

# Habilitar GPU (se dispon√≠vel)
# Ollama automaticamente usa GPU CUDA/Metal se dispon√≠vel
```

---

### Problema 5: Porta 11434 j√° est√° em uso

**Sintomas:**
```
Error: Address already in use
```

**Solu√ß√µes:**
```bash
# Verificar processo na porta
lsof -i :11434

# Matar processo anterior
pkill ollama

# Ou usar outra porta
ollama serve --port 11435

# Ajustar no .env
OLLAMA_URL=http://localhost:11435
```

---

### Problema 6: Modelos n√£o s√£o baixados

**Sintomas:**
- Comando `ollama pull` trava ou falha
- Erro de rede ou timeout

**Solu√ß√µes:**
```bash
# Verificar conex√£o com internet
ping ollama.com

# Usar proxy se necess√°rio
export HTTP_PROXY=http://proxy:porta
ollama pull llama3.2

# Baixar modelo manualmente e importar
# (veja documenta√ß√£o oficial do Ollama)
```

---

## FAQ

### 1. Posso usar Ollama em produ√ß√£o?

**Sim**, mas considere:
- ‚úÖ Hardware adequado (RAM, CPU/GPU)
- ‚úÖ Monitoramento de recursos
- ‚úÖ Backup com fallback para Gemini
- ‚úÖ Load balancing se alto volume

### 2. Qual a diferen√ßa de qualidade entre Ollama e Gemini?

**Gemini** geralmente tem respostas mais precisas e naturais (√© um modelo propriet√°rio SOTA). **Ollama** com llama3.2/mistral tem qualidade excelente para maioria dos casos, mas pode ser inferior em tarefas muito complexas.

### 3. Posso mudar de provider sem parar o servidor?

**N√£o.** √â necess√°rio:
1. Parar servidor Node.js
2. Alterar `AI_PROVIDER` no `.env`
3. Reiniciar servidor

**Dica:** Use o sistema h√≠brido para n√£o precisar mudar provider frequentemente.

### 4. O cache funciona entre providers?

**N√£o.** Cache do Ollama √© separado do cache do Gemini. Trocar de provider n√£o aproveita cache anterior.

### 5. Posso usar outros modelos al√©m dos padr√£o?

**Sim!** Veja modelos dispon√≠veis em [ollama.com/library](https://ollama.com/library).

Exemplo com GPT4All:
```bash
ollama pull gpt4all
```

No `.env`:
```bash
OLLAMA_TEXT_MODEL=gpt4all
```

### 6. Ollama suporta streaming?

**Sim**, mas est√° desabilitado na implementa√ß√£o atual por simplicidade. Para habilitar streaming, modifique `aiService.ollama.ts` e use `stream: true` nas requisi√ß√µes.

### 7. Como monitorar uso de recursos do Ollama?

```bash
# CPU e mem√≥ria em tempo real
top | grep ollama

# Logs detalhados
tail -f /tmp/ollama.log

# Estat√≠sticas de uso via API
curl http://localhost:5000/api/ai/stats
```

---

## üéØ Checklist de Setup Completo

- [ ] Instalar Ollama (`curl -fsSL https://ollama.com/install.sh | sh`)
- [ ] Baixar modelos (`ollama pull llama3.2 llava mistral`)
- [ ] Iniciar servidor (`ollama serve`)
- [ ] Configurar `.env` (AI_PROVIDER=ollama, OLLAMA_URL, etc.)
- [ ] Testar instala√ß√£o (`./server/scripts/test-ollama.sh`)
- [ ] Iniciar Node.js (`cd server && npm run dev`)
- [ ] Verificar health check (`curl .../api/ai/health`)
- [ ] Testar gera√ß√£o de texto
- [ ] Testar an√°lise de imagem
- [ ] Testar fallback (parar Ollama e verificar se usa Gemini)
- [ ] Monitorar logs e performance

---

## üìö Recursos Adicionais

- **Ollama Oficial:** https://ollama.com
- **Modelos Dispon√≠veis:** https://ollama.com/library
- **GitHub do Ollama:** https://github.com/ollama/ollama
- **Documenta√ß√£o API:** https://github.com/ollama/ollama/blob/main/docs/api.md

---

## üÜò Suporte

Se encontrar problemas n√£o listados neste guia:

1. Verifique logs do Ollama: `/tmp/ollama.log`
2. Verifique logs do Node.js: `server/dist/server.js`
3. Consulte issues no GitHub do projeto
4. Abra uma issue descrevendo o problema

---

**√öltima atualiza√ß√£o:** Novembro 2025
**Vers√£o do Ollama:** 0.1.0+
**Vers√£o do Mirai React:** Com suporte h√≠brido Ollama/Gemini
