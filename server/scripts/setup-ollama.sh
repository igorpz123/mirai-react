#!/bin/bash
# Script de instala√ß√£o e configura√ß√£o do Ollama
# Este script instala o Ollama e baixa os modelos necess√°rios para o sistema Mirai React

set -e  # Parar em caso de erro

echo "ü§ñ =========================================="
echo "   Instala√ß√£o do Ollama para Mirai React"
echo "=========================================="
echo ""

# Detectar sistema operacional
OS="$(uname -s)"
echo "üìã Sistema operacional detectado: $OS"
echo ""

# Instalar Ollama
if command -v ollama &> /dev/null; then
    echo "‚úì Ollama j√° est√° instalado"
    ollama --version
else
    echo "üì¶ Instalando Ollama..."
    
    if [[ "$OS" == "Linux" ]]; then
        curl -fsSL https://ollama.com/install.sh | sh
    elif [[ "$OS" == "Darwin" ]]; then
        echo "No macOS, baixe o instalador em: https://ollama.com/download"
        echo "Ou instale via Homebrew: brew install ollama"
        exit 1
    else
        echo "Sistema operacional n√£o suportado. Visite: https://ollama.com/download"
        exit 1
    fi
    
    echo "‚úì Ollama instalado com sucesso!"
fi

echo ""
echo "=========================================="
echo "üì• Baixando modelos necess√°rios..."
echo "=========================================="
echo ""

# Baixar modelo de texto (llama3.2 - ~2GB)
echo "1/3 Baixando llama3.2 (modelo de texto)..."
echo "   Tamanho: ~2GB"
ollama pull llama3.2
echo "‚úì llama3.2 baixado!"
echo ""

# Baixar modelo de vis√£o (llava - ~4.7GB)
echo "2/3 Baixando llava (modelo de vis√£o)..."
echo "   Tamanho: ~4.7GB"
ollama pull llava
echo "‚úì llava baixado!"
echo ""

# Baixar modelo alternativo (mistral - ~4GB)
echo "3/3 Baixando mistral (modelo alternativo)..."
echo "   Tamanho: ~4GB"
ollama pull mistral
echo "‚úì mistral baixado!"
echo ""

# Iniciar servidor Ollama (se n√£o estiver rodando)
echo "=========================================="
echo "üöÄ Iniciando servidor Ollama..."
echo "=========================================="
echo ""

if pgrep -x "ollama" > /dev/null; then
    echo "‚úì Servidor Ollama j√° est√° rodando"
else
    echo "Iniciando servidor em background..."
    ollama serve > /tmp/ollama.log 2>&1 &
    
    # Aguardar servidor iniciar
    sleep 3
    
    if pgrep -x "ollama" > /dev/null; then
        echo "‚úì Servidor Ollama iniciado com sucesso!"
        echo "   Logs em: /tmp/ollama.log"
    else
        echo "‚ö† Falha ao iniciar servidor automaticamente"
        echo "   Execute manualmente: ollama serve"
    fi
fi

echo ""
echo "=========================================="
echo "‚úÖ Instala√ß√£o conclu√≠da!"
echo "=========================================="
echo ""
echo "üéØ Pr√≥ximos passos:"
echo ""
echo "1. Configure as vari√°veis de ambiente no server/.env:"
echo "   AI_PROVIDER=ollama"
echo "   OLLAMA_URL=http://localhost:11434"
echo "   OLLAMA_TEXT_MODEL=llama3.2"
echo "   OLLAMA_VISION_MODEL=llava"
echo ""
echo "2. Teste a instala√ß√£o:"
echo "   ./server/scripts/test-ollama.sh"
echo ""
echo "3. Inicie seu servidor Node.js:"
echo "   cd server && npm run dev"
echo ""
echo "4. Verifique o health check:"
echo "   curl http://localhost:5000/api/ai/health"
echo ""
echo "üìö Documenta√ß√£o completa em: docs/ai/OLLAMA_SETUP.md"
echo ""
echo "üåê Servidor Ollama rodando em: http://localhost:11434"
echo "   Para parar: pkill ollama"
echo ""
